{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 lab: Elevator repair\n",
    "\n",
    "## Goals\n",
    "\n",
    "* Build a simple model based on synthetic count data: number of elevator failures per year in a building\n",
    "* Run inference in the model, and make decisions using posterior predictions\n",
    "\n",
    "## The plan\n",
    "\n",
    "* load synthetic data: number of elevator failures per year for your building and three nearby buildings\n",
    "* model data as a Poisson-distributed variable, using a Gamma distribution as a prior over the Poisson intensity\n",
    "* use posterior predictions about your model to ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pystan\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = pd.read_csv(\"elevator_failures.csv\")\n",
    "failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "\n",
    "Let's assume that the elevator failures happen at a constant probability per time of happening and are uncorrelated with each other- so they'll be described reasonbly well by a homogeneous Poisson process.\n",
    "\n",
    "The Poisson likelihood has one parameter ($\\lambda$) that is equal to both the mean and the variance of the distribution. $\\lambda$ has to be nonnegative but doesn't have an upper bound, so we'll use a Gamma distribution.\n",
    "\n",
    "Prior:\n",
    "\n",
    "$\\lambda \\sim gamma(a,b)$\n",
    "\n",
    "Likelihood:\n",
    "\n",
    "$y \\sim Poisson(\\lambda)$\n",
    "\n",
    "**Note:** numpy uses (`shape`, `scale`) to parameterize Gamma distribution; stan uses (`alpha`, `beta`) where `alpha=shape` and `beta=1/scale`. Example samples from a Gamma distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "b = 2\n",
    "prior_samples = np.random.gamma(a,b, size=1000)\n",
    "_ = plt.hist(prior_samples, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing samples from a prior\n",
    "\n",
    "If we have a likelihood $P(x|\\theta)$ and a prior $P(\\theta)$, then we can define a prior predictive distribution by marginalizing $\\theta$ out,\n",
    "\n",
    "$P(x) = \\int P(x|\\theta)P(\\theta)d\\theta$\n",
    "\n",
    "which we could then use to sample fake data. This won't be horribly interesting for such a simple model- but as we get to more complicated models, it can be a good early-warning system for catching mistakes. If the prior really does represent our prior knowledge, then sampling data using it should generate data that seems plausible (but broader than the distribution of data we know about). If we get data that looks implausible (for example, violating a law of physics or spread across many orders of magnitude more than the data) then chances are we've mis-specified our priors. From *Visualization in Bayesian workflow*,\n",
    "\n",
    "> As with the standard concept of weakly informative priors, it is important that this prior predictive distribution for the data has at least some mass around extreme but plausible data sets. On the other hand, there should be no mass on completely implausible data sets. We recommend assessing how informative the prior distribution on the data is by generating a “flip book” of simulated datasets that can be used to investigate the variability and multivariate structure of the distribution\n",
    "\n",
    "For a simple model like this it'd be easy to use `numpy` to generate prior predictive values- it's also possible to use `stan` for this; let's walk through the process so we can repeat it when we get to more interesting models.\n",
    "\n",
    "We'll specify prior hyperparameters as our data (which will let us play with them without recompiling the model each time) and use the `generated quantities` block to sample parameters and fake data. Note that no actual inference is happening within this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_model_code = \"\"\"\n",
    "data {\n",
    "    real alpha;\n",
    "    real beta;\n",
    "}\n",
    "model {\n",
    "}\n",
    "generated quantities {\n",
    "    real lambda;\n",
    "    int Y;\n",
    "    lambda = gamma_rng(alpha, beta);\n",
    "    Y = poisson_rng(lambda);\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_model = pystan.StanModel(model_code=prior_model_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"alpha\":2., \"beta\":0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we generate prior samples, we need to run `stan` with the keyword argument `algorithm='Fixed_param'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_samples = prior_model.sampling(data=data, iter=10000, chains=1, algorithm='Fixed_param')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(prior_samples.extract()[\"lambda\"], bins=50, density=True)\n",
    "plt.xlabel(\"$\\lambda$\", fontsize=14)\n",
    "plt.ylabel(\"$P(\\lambda)$\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(prior_samples.extract()[\"Y\"], bins=np.arange(10)-0.5, density=True)\n",
    "plt.xlabel(\"elevator malfunctions per year\", fontsize=14)\n",
    "plt.ylabel(\"prior predictive distribution\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build the stan model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_code = \"\"\"\n",
    "data {\n",
    "    // the data block defines the data structures where our data will go\n",
    "    // N is the number of observations- an integer with a lower-bound of 0\n",
    "    int<lower=0> N;\n",
    "    \n",
    "    // y is an array of the actual observations (integers with value 0 or 1), of length N\n",
    "    int<lower=0> y[N];\n",
    "}\n",
    "parameters {\n",
    "    // the parameters block defines the variables we're doing inference on- in\n",
    "    // this case, just lambda, the poisson \"intensity\" parameter. \n",
    "    \n",
    "    // for a poisson distribution, this parameter is both the expected mean\n",
    "    // and the standard deviation\n",
    "    real<lower=0> lam;\n",
    "}\n",
    "model {\n",
    "    // the model block ties the room together.\n",
    "    \n",
    "    // connect the lambda parameter to a prior distribution,\n",
    "    lam ~ gamma(2,0.5); // specify prior hyperparameters here\n",
    "    \n",
    "    // choose a likelihood to connect observations to the lambda parameter\n",
    "    for (n in 1:N)\n",
    "        y[n] ~ poisson(lam);\n",
    "}\n",
    "generated quantities {\n",
    "    // we can also have stan generate anything else we want- we'll use this a\n",
    "    // lot for posterior predictive checks. code in this block is evaluated \n",
    "    // once per sample.\n",
    "    \n",
    "    // so every time stan draws a value of lambda, let's also have it draw\n",
    "    // a simulated observation from that value\n",
    "    vector[N] y_sim;\n",
    "    \n",
    "    // generate posterior predictive samples\n",
    "    for(i in 1:N) {\n",
    "        y_sim[i] = poisson_rng(lam);\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = pystan.StanModel(model_code=model_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw samples from the posterior\n",
    "\n",
    "Run inference on the data from your building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"y\":failures[\"you\"], \"N\":len(failures)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = model.sampling(data=data, iter=10000, chains=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = fit.extract()[\"lam\"]\n",
    "posterior_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace plots\n",
    "\n",
    "Before we start interpreting results of our model, we should always check and make sure that MCMC *actually worked*. A standard diagnostic step is to evaluate the samples as a time series- if the goal is to generate random samples from the posterior, then the time series should look like random samples from a stationary distribution (as opposed to a line that slowly meanders around):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.title(\"trace for $\\lambda$\")\n",
    "plt.plot(posterior_samples)\n",
    "plt.subplot(122)\n",
    "plt.title(\"correlegram for $\\lambda$\")\n",
    "plt.plot(np.correlate(posterior_samples-posterior_samples.mean(), \n",
    "                      posterior_samples-posterior_samples.mean(), \"same\")[int(len(posterior_samples)/2):]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we want to see- consecutive samples are basically uncorrelated. Next week we'll look at a problematic Markov chain.\n",
    "\n",
    "Since MCMC looks like it worked as intended, let's compare the posterior distribution over $\\lambda$ to the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(prior_samples.extract()[\"lambda\"], bins=100, density=True, alpha=0.5, label=\"prior\")\n",
    "_ = plt.hist(posterior_samples, bins=100, density=True, alpha=0.5, label=\"posterior\")\n",
    "plt.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate summary statistics from the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the average observed value of $\\lambda?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures[\"you\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does our model estimate the average should be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(posterior_samples.mean(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 90% credible intervals- the region that the true value of $\\lambda$ has a 90% chance of lying within?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"95 percent: %s\"%np.round(np.quantile(posterior_samples, 0.95),2))\n",
    "print(\"5 percent: %s\"%np.round(np.quantile(posterior_samples, 0.05),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Predictive Checks\n",
    "\n",
    "> The idea behind posterior predictive checking is simple: if a model is a good fit we should\n",
    "be able to use it to generate data that resemble the data we observed.\n",
    "\n",
    "-from *Visualization in Bayesian workflow* by Gabry *et al*\n",
    "\n",
    "A final check we're going to do (to build some good Bayesian hygiene before we start building harder models) is the **posterior predictive check.** If our posterior distribution really is a good probabilistic description of our data, then samples from that distribution should look similar to our data.\n",
    "\n",
    "`y_sim` will be an array of size `(num_samples, num_observations)`; each row represents a simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sim = fit.extract()[\"y_sim\"]\n",
    "y_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sim.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,2,1)\n",
    "bins = np.arange(16) - 0.5\n",
    "_ = plt.hist(failures[\"you\"], bins=bins)\n",
    "plt.xlim(0,10)\n",
    "plt.title(\"observed data\")\n",
    "\n",
    "for i in range(3):\n",
    "    plt.subplot(2,2,i+2)\n",
    "    j = np.random.randint(y_sim.shape[0])\n",
    "    _ = plt.hist(y_sim[j,:], bins=bins)\n",
    "    plt.xlim(0,10)\n",
    "    plt.title(\"posterior predictive sample\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: use your model to make an actual decision\n",
    "\n",
    "Let's say that the elevator company sells service contracts for repairing broken elevators:\n",
    "\n",
    "* You can prepay for $N$ service visits per year, at a cost of $N \\times $ \\$1000 (whether or not you use them all)\n",
    "* Non-prepaid service visits cost \\$1500\n",
    "\n",
    "**Use your model to predict the value of $N$ that will minimize expected cost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggested approach: write a function that inputs the number of prepaid visits and the number of failures for a given year, and outputs the total cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(num_failures, n):\n",
    "    # REMOVE CODE FOR LAB\n",
    "    if num_failures <= n:\n",
    "        return n*1000\n",
    "    else:\n",
    "        return n*1000 + (num_failures-n)*1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function_returns_correct_values():\n",
    "    # unit test for cost_function()\n",
    "    assert cost_function(0,1) == 1000\n",
    "    assert cost_function(1,1) == 1000\n",
    "    assert cost_function(3,2) == 3500\n",
    "    \n",
    "cost_function_returns_correct_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior samples we drew are values of $\\lambda$, but we need actual failure counts for our cost function. For any $\\lambda_{i} \\sim P(\\lambda |x)$, we can draw a failure count $y_{i} \\sim Poisson(\\lambda_{i})$ (that is, sampling from the \"posterior predictive distribution\").\n",
    "\n",
    "We can use these samples with our cost function to estimate the expected cost of different choices for $N$ (you can do this brute-force style with a `for` loop or two; it doesn't have to be pretty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE CODE FOR LAB\n",
    "expected_costs = []\n",
    "\n",
    "for i in range(10):\n",
    "    expectation_value = np.mean([cost_function(np.random.poisson(lam),i) for lam in posterior_samples])\n",
    "    expected_costs.append(expectation_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just look for a minimum in our expected cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(expected_costs, \"o-\")\n",
    "plt.xlabel(\"N\", fontsize=14)\n",
    "plt.ylabel(\"expected cost\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: reuse your analysis to answer a harder question\n",
    "\n",
    "Our original dataset contained the historical counts of elevator failures in your building, as well as buildings where three of your friends work. Could you save money if the four of you combined forces with a single shared service plan, or are you better off with four separate plans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE CODE FOR THIS ENTIRE SECTION FOR LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cost = {\"you\":np.min(expected_costs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [\"friend1\", \"friend2\", \"friend3\"]:\n",
    "    fit = model.sampling(data={\"y\":failures[f], \"N\":len(failures)}, iter=10000, chains=4)\n",
    "    posterior_samples = fit.extract()[\"lam\"]\n",
    "    expected_costs_f = []\n",
    "\n",
    "    for i in range(10):\n",
    "        expectation_value = np.mean([cost_function(np.random.poisson(lam),i) for lam in posterior_samples])\n",
    "        expected_costs_f.append(expectation_value)\n",
    "        \n",
    "    min_cost[f] = np.min(expected_costs_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_failures = failures.values.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = model.sampling(data={\"y\":total_failures, \"N\":len(failures)}, iter=10000, chains=4)\n",
    "posterior_samples = fit.extract()[\"lam\"]\n",
    "expected_costs_s = []\n",
    "\n",
    "for i in range(10):\n",
    "    expectation_value = np.mean([cost_function(np.random.poisson(lam),i) for lam in posterior_samples])\n",
    "    expected_costs_s.append(expectation_value)\n",
    "    \n",
    "shared_min_cost = np.min(expected_costs_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPECTED COST FOR RISK POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_min_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPECTED COST FOR INDEPENDENT SERVICE CONTRACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(list(min_cost.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
