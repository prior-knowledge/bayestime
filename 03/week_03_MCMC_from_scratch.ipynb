{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 lab: Markov-Chain Monte Carlo\n",
    "\n",
    "## Goal\n",
    "\n",
    "Gain a better understanding of what's going on under the hood in MCMC, so you'll be better at diagnosing and fixing computational problems when they occur (and they *will* occur).\n",
    "\n",
    "## The plan\n",
    "\n",
    "1. Simulate some data from a polynomial with noise\n",
    "* Define a model for a Bayesian polynomial regression\n",
    "* Implement Metropolis-Hastings from scratch, like your grandparents did\n",
    "  * We've provided basic derivations for all the quantities you'll need to compute\n",
    "  * We've laid out the functions you'll need to write- each one shouldn't be more than 10 or so lines\n",
    "  * For each function, we've written a unit test you can run. It won't ensure your code is completely right, but it'll make sure it's at least outputting the right types of things\n",
    "* Critically evaluate your MH outputs to see if you can get a reasonable answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic polynomial data\n",
    "xvals = np.linspace(0,5,1000)\n",
    "coefs = np.array([1, -0.4, 0.05])\n",
    "yvals = coefs[0]*xvals + coefs[1]*xvals**2 + coefs[2]*xvals**3\n",
    "\n",
    "np.random.seed(1)\n",
    "N = 100\n",
    "x = np.random.uniform(0, 5, size=N)\n",
    "y = coefs[0]*x + coefs[1]*x**2 + coefs[2]*x**3 + np.random.normal(0, 0.1, size=N)\n",
    "\n",
    "\n",
    "plt.plot(xvals, yvals, label=\"true line\")\n",
    "plt.plot(x, y, \"o\", label=\"observed data\", alpha=0.5)\n",
    "plt.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "Here's a simple model we could use (that happens to also be the one the data was generated by). Our predictions $\\hat{y}$ are from a polynomial:\n",
    "\n",
    "$\\hat{y}_{i} = \\theta_{1}x_{i} + \\theta_{2}x_{i}^{2} + \\theta_{3}x_{i}^{3}$\n",
    "\n",
    "Where we place a normally-distributed prior over each of the $\\theta$s,\n",
    "\n",
    "$\\theta_{j} \\sim N(0, \\sigma_{j}^{2})$\n",
    "\n",
    "and we expect the actual values are normally distributed (with standard deviation 1) around the prediction:\n",
    "\n",
    "$y_{i} \\sim N(\\hat{y}, 1)$\n",
    "\n",
    "### What else would we include in a \"real\" regression model?\n",
    "\n",
    "I left a couple things out of this example problem for simplicity. Generally we'd have:\n",
    "\n",
    "* a parameter for the intercept\n",
    "* a parameter for the variance around the prediction, instead of assuming that it's 1\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "We'll need to specify three hyperparameters- the standard deviations of the prior for each of the three regression coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = np.array([5,5,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding up Metropolis-Hastings\n",
    "\n",
    "### Refresher: what does MH do?\n",
    "\n",
    "Start with some value for our parameters $\\theta_{0}$. Each iteration $t$,\n",
    "\n",
    "1. Sample a **candidate value** $\\theta^{'}$ near $\\theta_{t}$\n",
    "2. Compute $f(\\theta)$ for both $\\theta_{t}$ and $\\theta^{'}$, where $f$ is some function $f(\\theta) \\propto P(\\theta|y)$\n",
    "3. Compute the **acceptance ratio** $\\alpha = f(\\theta^{'})/f(\\theta_{t})$\n",
    "4. Sample a random number on the unit interval, $u \\sim Uniform(0,1)$\n",
    "5. Decide whether to accept or reject. \n",
    "\n",
    "  * If $u \\le \\alpha$: **accept**; $\\theta_{t+1}=\\theta^{'}$\n",
    "  * If $u \\gt \\alpha$: **reject**; $\\theta_{t+1}=\\theta_{t}$\n",
    "\n",
    "\n",
    "### The plan\n",
    "\n",
    "We're drawing samples from a function\n",
    "\n",
    "$P(\\theta|y) \\propto P(y|\\theta)P(\\theta)$\n",
    "\n",
    "So for the MH accept/reject decision we need to compute $P(y|\\theta)P(\\theta)$ for each proposed value of our regression parameters $\\theta$. Since the MH acceptance depends on a ratio,\n",
    "\n",
    "$\\alpha = \\frac{P(y|\\theta^{'})P(\\theta^{'})}{P(y|\\theta)P(\\theta)}$\n",
    "\n",
    "we can ignore any proportionality constants. It's also generally useful, when computing with probabilities, to use logarithms so that we don't get rounding errors for values close to zero. So what we'll actually compute (remembering that $\\log[a/b] = \\log[a]-\\log[b]$) is\n",
    "\n",
    "$\\log[\\alpha] = \\log[P(y|\\theta^{'})P(\\theta^{'})] - \\log[P(y|\\theta)P(\\theta)]$\n",
    "\n",
    "### Prior\n",
    "\n",
    "The prior is a normal distribution,\n",
    "\n",
    "$P(\\theta) = P(\\theta_{1})P(\\theta_{2})P(\\theta_{3}) \\propto \\prod_{j=1}^{3}exp\\left(-\\frac{1}{2} \\left(\\frac{\\theta_{j}}{\\sigma_{j}}\\right)^{2} \\right)$\n",
    "\n",
    "where each $\\sigma_{j}$ is the prior hyperparameter for the $j$th parameter. By making any of the hyperparameters smaller, we can encode prior knowledge that that parameter should be close to zero. Our unnormalized log probability is\n",
    "\n",
    "$\\log[P(\\theta)] = -\\sum_{j=1}^{3}\\frac{1}{2} \\left(\\frac{\\theta_{j}}{\\sigma_{j}}\\right)^{2} $\n",
    "\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "The likelihood is also a normal distribution (of $y$ this time). If we define $\\hat{y}_{i}(\\theta)$ to be the prediction for observation $i$ and parameters $\\theta$, then (since we've set the variance to one)\n",
    "\n",
    "$P(y|\\theta) = \\prod_{i=1}^{N}P(y_{i}|\\theta) \\propto \\prod_{i}exp\\left(-\\frac{1}{2} (y_{i} - \\hat{y}_{i}(\\theta))^{2} \\right)$\n",
    "\n",
    "So the unnormallized log likelihood is\n",
    "\n",
    "$\\log[P(y|\\theta)] = -\\sum_{i=1}^{N} \\frac{1}{2} (y_{i} - \\hat{y}_{i}(\\theta))^{2} $\n",
    "\n",
    "### Putting it all together\n",
    "\n",
    "Every MH update, you'll decide to accept or reject based on whether $\\log[u]$ is greater or less than\n",
    "\n",
    "$\\left[ -\\sum_{i=1}^{N} \\frac{1}{2} (y_{i} - \\hat{y}_{i}(\\theta^{'}))^{2} -\\sum_{j=1}^{3}\\frac{1}{2} \\left(\\frac{\\theta_{j}^{'}}{\\sigma_{j}}\\right)^{2}\\right] - \n",
    "\\left[ -\\sum_{i=1}^{N} \\frac{1}{2} (y_{i} - \\hat{y}_{i}(\\theta))^{2} -\\sum_{j=1}^{3}\\frac{1}{2} \\left(\\frac{\\theta_{j}}{\\sigma_{j}}\\right)^{2}\\right]$\n",
    "\n",
    "### Your mission\n",
    "\n",
    "* write a function `pred()` that inputs a value of $\\theta$ and returns predictions $\\hat{y}_{i}(\\theta)$ for all $i$\n",
    "* write a function `log_prob()` that inputs a value of $\\theta$ and returns the unnormalized log probability. It will need to call `pred()` to compute the likelihood\n",
    "* write a function `metropolis_hastings_update()` that inputs a value of $\\theta$ and a jump distance, and returns three things:\n",
    "  * the next value of $\\theta$\n",
    "  * the corresponding log-probability (which we'll use to tune our burn-in)\n",
    "  * a Boolean that's `True` if this was an acceptance or `False` if it was a rejection (which we'll use to tune the jump distance)\n",
    "  \n",
    "A couple things you may need:\n",
    "\n",
    "* `np.random.normal(0,jump_distance,3)` to generate a Gaussian hop for a 3D array\n",
    "* `np.random.uniform(0,1)` to generate a random number on the unit interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(theta):\n",
    "    \"\"\"\n",
    "    Input array of coeffecients; output array of predicted y-hat values\n",
    "    \"\"\"\n",
    "    # DELETE CODE FOR THE LAB\n",
    "    return theta[0]*x + theta[1]*x**2 + theta[2]*x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pred_correct_value():\n",
    "    # unit test for pred()\n",
    "    # does it give right answers for trivial cases?\n",
    "    assert (pred(np.array([1,0,0])) == x).all()\n",
    "    assert (pred(np.array([0,1,0])) == x**2).all()\n",
    "    assert (pred(np.array([0,0,1])) == x**3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_correct_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob(theta):\n",
    "    \"\"\"\n",
    "    Input a value for the coeffecients and return the (unnormalized) log-probability.\n",
    "    \"\"\"\n",
    "    # DELETE CODE FOR THE LAB\n",
    "    return -0.5*np.sum((pred(theta)-y)**2) - 0.5*np.sum((theta/hyperparams)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_log_prob():\n",
    "    # unit test for log_prob\n",
    "    # does it return the right type of thing?\n",
    "    th = np.array([1,1,1])\n",
    "    assert isinstance(log_prob(th), float)\n",
    "    # test an obvious case\n",
    "    assert log_prob(np.array([100,100,100])) < log_prob(np.array([0,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_prob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings_update(theta, jump_distance):\n",
    "    \"\"\"\n",
    "    Perform an update step for Metropolis hastings.\n",
    "    \n",
    "    :theta: numpy array of initial parameters\n",
    "    :jump_distance: standard deviation of normal distribution for\n",
    "        sampling candidate values\n",
    "        \n",
    "    Returns\n",
    "        -New value of theta\n",
    "        -Corresponding unnormalized log-probability\n",
    "        -Boolean indicating whether this was an acceptance (True) or rejection (False)\n",
    "    \"\"\"\n",
    "    # DELETE CODE FOR THE LAB\n",
    "    # sample a candidate value for theta\n",
    "    theta_prime = theta + np.random.normal(0, jump_distance, 3)\n",
    "    # compute log probs\n",
    "    old_prob = log_prob(theta)\n",
    "    new_prob = log_prob(theta_prime)\n",
    "    # compute log acceptance ratio\n",
    "    log_acceptance_ratio = new_prob - old_prob\n",
    "    # sample our acceptance threshold\n",
    "    u = np.random.uniform(0,1)\n",
    "    # accept\n",
    "    if np.log(u) <= log_acceptance_ratio:\n",
    "        return theta_prime, new_prob, True\n",
    "    # reject\n",
    "    else:\n",
    "        return theta, old_prob, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metropolis_hastings_update():\n",
    "    # just make sure it runs and outputs the right types of stuff\n",
    "    th, p, a = metropolis_hastings_update(np.zeros(3), 1e-3)\n",
    "    assert isinstance(th, np.ndarray)\n",
    "    assert len(th) == 3\n",
    "    assert isinstance(p, float)\n",
    "    assert isinstance(a, bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metropolis_hastings_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now wrap it up and put a bow on it\n",
    "\n",
    "Finally, write a function to manage the boring details of actually running MCMC:\n",
    "\n",
    "* input a starting value for $\\theta$ and any sampling parameters\n",
    "* run MCMC\n",
    "* manage and return the parameter values at every iteration, the log probability, and acceptance ratio\n",
    "\n",
    "#### If you want to be fancy\n",
    "\n",
    "Use `tqdm` to add a progress bar. Usage looks like this:\n",
    "\n",
    "```\n",
    "from tqdm import tqdm\n",
    "\n",
    "for n in tqdm(range(N)):\n",
    "    # stuff here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(N=10000, init=np.zeros(3), jump_dist=5e-3):\n",
    "    \"\"\"\n",
    "    Generate MCMC samples for our model\n",
    "    \n",
    "    :N: number of iterations\n",
    "    :init: 3-dimensional numpy array of initial values\n",
    "    :jump_dist: standard deviation of normal distribution used for candidate generation\n",
    "    \n",
    "    Returns\n",
    "        -thetas: (N,3) array of parameter samples\n",
    "        -probs: (N,) array of log-probabilities\n",
    "        -acceptance_ratio: float; fraction of candidates accepted\n",
    "    \"\"\"\n",
    "    # DELETE CODE FOR THE LAB\n",
    "    thetas = np.zeros((N,3))\n",
    "    probs = np.zeros((N,))\n",
    "    accepts = 0\n",
    "    theta = init\n",
    "    \n",
    "    for n in tqdm(range(N)):\n",
    "        theta, l, a = metropolis_hastings_update(theta, jump_dist)\n",
    "        thetas[n,:] = theta\n",
    "        probs[n] = l\n",
    "        accepts += int(a)\n",
    "        \n",
    "    return thetas, probs, accepts/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metropolis_hastings():\n",
    "    # unit test- just make sure MH runs and gives right output types\n",
    "    th, l, a = metropolis_hastings(1)\n",
    "    assert isinstance(th, np.ndarray)\n",
    "    assert th.shape == (1,3)\n",
    "    assert isinstance(l, np.ndarray)\n",
    "    assert l.shape == (1,)\n",
    "    assert isinstance(a, float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metropolis_hastings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference and examine results\n",
    "\n",
    "You can use the rest of this lab to experiment with your very own bespoke MCMC sampler. Below are some steps we can take to interpret the outputs of the sampler; try running under different configurations to see how good you can get your results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas, probs, AR = metropolis_hastings(100000, jump_dist=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acceptance ratio\n",
    "\n",
    "The **acceptance ratio** (fraction of proposals that were accepted) gives us some intuition for how to set `jump_dist`. If `AR` is close to 0, then it means we're wasting a lot of time testing impractical proposals and just reiterating the same values- so we should be taking smaller steps.\n",
    "\n",
    "If `AR` is close to 1, it probably means we're taking steps that are too small and we're not fully exploring the parameter space.\n",
    "\n",
    "Gelman says to aim for around 0.23.\\*\n",
    "\n",
    "\n",
    "\\**WEAK CONVERGENCE AND OPTIMAL SCALING OF RANDOM WALK METROPOLIS ALGORITHMS* by Roberts, Gelman and Gilks, The Annals of Applied Probability 1997, Vol. 7, No. 1, 110-120)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-probability\n",
    "\n",
    "If we don't put much thought into initialization (and we didn't), the sampler might take a while to converge- the initial samples could be far from the actual posterior distribution. The log-probability can give us some insight into how many samples we should prune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(probs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter values\n",
    "\n",
    "It's always a good idea to plot the values of the different parameters \"over time\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.subplot(3,1,i+1)\n",
    "    plt.plot(thetas[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: what are we looking for?\n",
    "\n",
    "Metropolis Hastings (if everything's working right) *asymptotically* converges to the distribution being simulated. If we're in (or close to) that asymptote, the samples should look like independent draws from a stationary distribution; if not, they'll be autocorrelated.\n",
    "\n",
    "Let's compare a stationary distribution (IID draws from normal distribution) with an autocorrelated distribution (a Gaussian random walk):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_from_a_distribution = np.random.normal(0,1,10000)\n",
    "random_walk = samples_from_a_distribution.cumsum()\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(samples_from_a_distribution)\n",
    "plt.title(\"this is a stationary distribution\")\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.title(\"this is not\")\n",
    "plt.plot(random_walk)\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the autocorrelation directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation(x):\n",
    "    x_mean_shifted = x - x.mean()\n",
    "    return np.correlate(x_mean_shifted, x_mean_shifted, \"same\")[int(len(x)/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(samples_from_a_distribution)\n",
    "plt.subplot(121)\n",
    "plt.title(\"correlegram of\\nuncorrelated samples\")\n",
    "plt.plot(autocorrelation(samples_from_a_distribution))\n",
    "plt.subplot(122)\n",
    "plt.title(\"correlegram of\\nautocorrelated samples\")\n",
    "plt.plot(autocorrelation(random_walk));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the outputs of our MCMC sampler- how do they look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = thetas.shape[0]\n",
    "\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.plot(autocorrelation(thetas[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some options for how to address this:\n",
    "\n",
    "* specify a \"burn-in\" phase where we discard the first samples (look at the log-prob plot to see how long this phase should be)\n",
    "* \"thin\" the samples to make them less autocorrelated by keeping only every $n$th sample\n",
    "* adjust sampling parameters\n",
    "* collect more samples (but please make sure you have some options handy other than \"brute force\")\n",
    "* check for correlations between parameters; you may need a different sampling algorithm\n",
    "* put more thought into how we choose our initial parameter values- in more complicated models poor initialization can completely prevent the model from converging\n",
    "* most importantly: **consider adjusting your model.** Remember the....\n",
    "\n",
    "### Folk theorem of statistical computing\n",
    "\n",
    "*When you have computational problems, often there's a problem with your model* (Gelman)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burn-in and thinning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = 20000\n",
    "thin = 10\n",
    "\n",
    "for i in range(3):\n",
    "    plt.subplot(3,1,i+1)\n",
    "    plt.plot(thetas[burnin::thin,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a histogram of the sampled values will give a picture of the marginal posterior of each parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.hist(thetas[burnin::thin,i], bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a few scatter plots to visualize any correlations between the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin = 20000\n",
    "thin = 10\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    j = (i+1)%3\n",
    "    plt.plot(thetas[burnin::thin,i], thetas[burnin::thin,(i+1)%3], \".\", alpha=0.05)\n",
    "    plt.title(\"$\\\\theta_{%s}$ vs $\\\\theta_{%s}$\"%(j+1,i+1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the **priors** for our parameters are independent, the **posteriors** are highly correlated- conditioning on our data has added an interaction between all of them. So choosing a particular value for $\\theta_{1}$ changes the maximum-posterior values for $\\theta_{2}$ and $\\theta_{3}$.\n",
    "\n",
    "Notice that our implementation of MH didn't take these correlations into account anywhere- when we propose a new value for the parameters, it's guessing randomly in parameter space even though an increase to $\\theta_{1}$ should generally mean a decrease in $\\theta_{2}$. More-sophisticated MCMC samplers (like HMC and NUTS) will sample more efficiently by factoring in these relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yes, but does it work?\n",
    "\n",
    "How close we need our samples to being \"truly stationary\" will depend on the application- even with our current rough-around-the-edges Markov chain, we can check to see whether it's learning the basic structure of our data and faithfully representing uncertainty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xvals, yvals, label=\"true line\")\n",
    "plt.plot(x, y, \"o\", label=\"observed data\", alpha=0.5)\n",
    "\n",
    "for i in list(range(len(probs)))[burnin::500][:250]:\n",
    "    y_post = thetas[i,0]*xvals + thetas[i,1]*xvals**2 + thetas[i,2]*xvals**3\n",
    "    plt.plot(xvals, y_post, \"r\", alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
